{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f83a9118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import heapq\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9739a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_update.txt', 'r', encoding='utf-8') as file:\n",
    "    training_data = file.read()\n",
    "\n",
    "\n",
    "\n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(\"[^a-zA-Zñáéíóúü ]\", \" \", newString)\n",
    "    newString = re.sub('\\s+', ' ', newString)\n",
    "    long_words = [word for word in newString.split() if len(word) >= 1]\n",
    "    return \" \".join(long_words).strip()\n",
    "\n",
    "def split_into_segments(input_data):\n",
    "    segments = input_data.split('</entry>')\n",
    "    cleaned_segments = [re.sub(r'<.*?>', '', segment).strip() for segment in segments if segment.strip()]\n",
    "    return cleaned_segments\n",
    "\n",
    "\n",
    "segments = split_into_segments(training_data)\n",
    "cleaned_segments = [text_cleaner(segment) for segment in segments]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000e9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenizing the text into words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([cleaned_segments])\n",
    "encoded = tokenizer.texts_to_sequences([cleaned_segments])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8898721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Creating Sequences of Tokens\n",
    "sequence_length = 5  # Length of the word sequences\n",
    "sequences = [encoded[i - sequence_length:i+1] for i in range(sequence_length, len(encoded))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd09acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Preparing the dataset\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc7d583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Defining the Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=sequence_length, trainable=True))\n",
    "model.add(GRU(150, return_sequences=False))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c67909a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "44/44 [==============================] - 2s 12ms/step - loss: 7.8001 - accuracy: 0.0011 - val_loss: 7.7245 - val_accuracy: 0.0000e+00 - lr: 0.0100\n",
      "Epoch 2/50\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 7.3261 - accuracy: 0.0120 - val_loss: 7.6226 - val_accuracy: 0.0058 - lr: 0.0100\n",
      "Epoch 3/50\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 5.3781 - accuracy: 0.1694 - val_loss: 6.8793 - val_accuracy: 0.1435 - lr: 0.0100\n",
      "Epoch 4/50\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 2.5936 - accuracy: 0.4846 - val_loss: 6.4801 - val_accuracy: 0.4232 - lr: 0.0100\n",
      "Epoch 5/50\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 0.7010 - accuracy: 0.8825 - val_loss: 6.8917 - val_accuracy: 0.4493 - lr: 0.0100\n",
      "Epoch 6/50\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 0.1614 - accuracy: 0.9771 - val_loss: 7.0544 - val_accuracy: 0.4551 - lr: 0.0100\n",
      "Epoch 7/50\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 0.0295 - accuracy: 0.9971 - val_loss: 7.1391 - val_accuracy: 0.4565 - lr: 0.0100\n",
      "Epoch 8/50\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 7.1454 - val_accuracy: 0.4565 - lr: 0.0020\n",
      "Epoch 9/50\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 7.1521 - val_accuracy: 0.4565 - lr: 0.0020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x205f3c17160>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Training\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5),\n",
    "    ModelCheckpoint('model.h5', save_best_only=True, monitor='val_loss'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
    "]\n",
    "\n",
    "model.fit(X_tr, y_tr, epochs=50, batch_size=64, validation_data=(X_val, y_val), verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "336a0032",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the texts before the gap: Jueves veinte de\n",
      "Enter the texts after the gap: mil setecientos\n",
      "jueves veinte de sebastiana aos vinte e tres de abril de mil e seiscentos e oitenta e sete baptizei e pus os santos oelos a sebastiana filha de miguel e de sua mulher ignacia nunes for o padrinhos domingos correa e da matheus de numo sábado dies de mayo de mil ochocientos setenta y pablo macario her nandes tres yo presbítero dn santiago serra cura párroco vicario forá neo interino de esta yglesia de término de san cárlos de ma tánzas bauticé solemnemente y puse los sántos óleos á un párvulo que nació el dia quince de enero último hijo de padre no conocido y de la morena libre simona hernan des natural y vecina de esta feligresía nieto materno de tomas y de julia hernandes en dicho párvulo ejercí las sagrádas préces y ceremónias y le puse por nombre pa blo macario fueron sus padrinos blas martines y ba silia delfi á quienes advertí el parentesco espiritual y o bligaciones que contrajeron y lo firmé sant serra numo maría de la ca ridad yzquierdo de bargallo sábado dies de mayo de mil ochocientos setenta y tres yo presbítero dn santiago serra cura párroco vicario fo ráneo interino de esta yglesia de término de san cár los de matánzas bauticé solemnemente y puse los sán tos óleos á una párvula morena que nació el dia veinte y ocho de abril último hija de padre no conocido y de la de igual clase manuela conga y vecina de esta feligre sía esclava de da rafaela yzquierdo vuida de bar gallo en dicha párvula ejercí las sagradas préces y cere monias y le puse por nombre maría de la caridad la cual es libre por beneficio de la ley sancionada por las córtes constituyentes de la nacion en veinte y tres de junio de mil ochocientos setenta años comunicada con fecha cuatro de julio del propio al escelentisimo se ñor gobernador superior político de esta ysla publica da en la gaceta oficial correspondiente al sábado prime ro de octubre del referido año fué su madrina da pe tronila dias á quien advertí el parentesco espiritual y obligaciones que contrajeron y lo firmé entre lí neas morena vale sant serra jo o em o primeiro dia de junho de mil e seiscentos e oitenta e nove baptizei em caza por necessidade a jo o filho de sebasti o ferraz e de sua molher lionor rodrigueaz for o padrinhos antonio vieira da cunha e sua molher hieronima ferraz e a dous de outubro lhe fiz na igreja o oficio e pus os santos oleos mesmos padrinhos matheus da silveira mil setecientos\n"
     ]
    }
   ],
   "source": [
    "def generate_text_for_gap(model, tokenizer, sequence_length, text_before_gap, text_after_gap, gap_length_estimate=3, beam_width=3):\n",
    "    #lower case\n",
    "    text_before_gap = text_before_gap.lower()\n",
    "    text_after_gap = text_after_gap.lower()\n",
    "\n",
    "    seed_text = text_before_gap\n",
    "    sequence_seed = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    seed_text_length = len(sequence_seed)\n",
    "\n",
    "    results = generate_seq_beam_search(model, tokenizer, sequence_length, sequence_seed, gap_length_estimate, beam_width)\n",
    "    best_sequence = results[0]['seq']\n",
    "\n",
    "    predicted_gap_content = ' '.join(tokenizer.index_word.get(idx, '') for idx in best_sequence[seed_text_length:])\n",
    "\n",
    "    completed_sentence = text_before_gap + ' ' + predicted_gap_content + ' ' + text_after_gap\n",
    "    return completed_sentence\n",
    "\n",
    "text_before_gap = input(\"Enter the texts before the gap: \")\n",
    "text_after_gap = input(\"Enter the texts after the gap: \")\n",
    "generated_text = generate_text_for_gap(model, tokenizer, sequence_length, text_before_gap, text_after_gap)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
