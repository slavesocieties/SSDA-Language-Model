{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5fc2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import heapq\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a72670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Text Cleaner\n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zñáéíóúü]\", \" \", newString)\n",
    "    long_words=[]\n",
    "    for i in newString.split():\n",
    "        if len(i)>=3:\n",
    "            long_words.append(i)\n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e7e138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('training_data.txt', 'r', encoding='utf-8') as file:\n",
    "    training_data = file.read()\n",
    "\n",
    "data_new = text_cleaner(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9736604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Creating Sequences\n",
    "def create_seq(text, length = 30):\n",
    "    sequences = list()\n",
    "    for i in range(length, len(text)):\n",
    "        seq = text[i-length:i+1]\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "sequences = create_seq(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450abd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Character Mapping\n",
    "chars = sorted(list(set(data_new)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "reverse_mapping = {i: c for c, i in mapping.items()}  # Reverse mapping for efficiency\n",
    "\n",
    "def encode_seq(seq):\n",
    "    sequences = list()\n",
    "    for line in seq:\n",
    "        encoded_seq = [mapping[char] for char in line]\n",
    "        sequences.append(encoded_seq)\n",
    "    return sequences\n",
    "\n",
    "sequences = encode_seq(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a14eca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Preparing the dataset\n",
    "vocab = len(mapping)\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# 5. Defining the Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
    "model.add(GRU(150, dropout=0.1))\n",
    "model.add(Dense(vocab, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer=Adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c69ed232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3960/3960 [==============================] - 33s 7ms/step - loss: 2.0249 - acc: 0.3872 - val_loss: 2.1130 - val_acc: 0.3496 - lr: 0.0100\n",
      "Epoch 2/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 2.0996 - acc: 0.3480 - val_loss: 2.0491 - val_acc: 0.3702 - lr: 0.0100\n",
      "Epoch 3/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 2.1223 - acc: 0.3395 - val_loss: 2.0597 - val_acc: 0.3584 - lr: 0.0100\n",
      "Epoch 4/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 2.1174 - acc: 0.3406 - val_loss: 2.0585 - val_acc: 0.3668 - lr: 0.0100\n",
      "Epoch 5/50\n",
      "3960/3960 [==============================] - 28s 7ms/step - loss: 2.1075 - acc: 0.3495 - val_loss: 2.0695 - val_acc: 0.3585 - lr: 0.0100\n",
      "Epoch 6/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9897 - acc: 0.3834 - val_loss: 1.9174 - val_acc: 0.4119 - lr: 0.0020\n",
      "Epoch 7/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9693 - acc: 0.3924 - val_loss: 1.9157 - val_acc: 0.4170 - lr: 0.0020\n",
      "Epoch 8/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9563 - acc: 0.3961 - val_loss: 1.9002 - val_acc: 0.4253 - lr: 0.0020\n",
      "Epoch 9/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9576 - acc: 0.3950 - val_loss: 1.9086 - val_acc: 0.4143 - lr: 0.0020\n",
      "Epoch 10/50\n",
      "3960/3960 [==============================] - 28s 7ms/step - loss: 1.9603 - acc: 0.3936 - val_loss: 1.9098 - val_acc: 0.4163 - lr: 0.0020\n",
      "Epoch 11/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9558 - acc: 0.3950 - val_loss: 1.8880 - val_acc: 0.4188 - lr: 0.0020\n",
      "Epoch 12/50\n",
      "3960/3960 [==============================] - 26s 7ms/step - loss: 1.9530 - acc: 0.3959 - val_loss: 1.8966 - val_acc: 0.4198 - lr: 0.0020\n",
      "Epoch 13/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9553 - acc: 0.3921 - val_loss: 1.9051 - val_acc: 0.4087 - lr: 0.0020\n",
      "Epoch 14/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9580 - acc: 0.3911 - val_loss: 1.8872 - val_acc: 0.4233 - lr: 0.0020\n",
      "Epoch 15/50\n",
      "3960/3960 [==============================] - 28s 7ms/step - loss: 1.9478 - acc: 0.3968 - val_loss: 1.8935 - val_acc: 0.4229 - lr: 0.0020\n",
      "Epoch 16/50\n",
      "3960/3960 [==============================] - 28s 7ms/step - loss: 1.9574 - acc: 0.3925 - val_loss: 1.8967 - val_acc: 0.4197 - lr: 0.0020\n",
      "Epoch 17/50\n",
      "3960/3960 [==============================] - 28s 7ms/step - loss: 1.9622 - acc: 0.3932 - val_loss: 1.9099 - val_acc: 0.4111 - lr: 0.0020\n",
      "Epoch 18/50\n",
      "3960/3960 [==============================] - 28s 7ms/step - loss: 1.9481 - acc: 0.3985 - val_loss: 1.8880 - val_acc: 0.4198 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "3960/3960 [==============================] - 28s 7ms/step - loss: 1.9476 - acc: 0.3962 - val_loss: 1.8871 - val_acc: 0.4232 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9448 - acc: 0.3971 - val_loss: 1.8882 - val_acc: 0.4189 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9415 - acc: 0.3995 - val_loss: 1.8798 - val_acc: 0.4236 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9354 - acc: 0.4034 - val_loss: 1.8656 - val_acc: 0.4300 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9289 - acc: 0.4051 - val_loss: 1.8660 - val_acc: 0.4322 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9252 - acc: 0.4057 - val_loss: 1.8622 - val_acc: 0.4330 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9238 - acc: 0.4076 - val_loss: 1.8538 - val_acc: 0.4345 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9250 - acc: 0.4061 - val_loss: 1.8558 - val_acc: 0.4348 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "3960/3960 [==============================] - 28s 7ms/step - loss: 1.9259 - acc: 0.4071 - val_loss: 1.8617 - val_acc: 0.4333 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9239 - acc: 0.4074 - val_loss: 1.8615 - val_acc: 0.4302 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "3960/3960 [==============================] - 27s 7ms/step - loss: 1.9275 - acc: 0.4039 - val_loss: 1.8561 - val_acc: 0.4344 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "3960/3960 [==============================] - 28s 7ms/step - loss: 1.9294 - acc: 0.4031 - val_loss: 1.8616 - val_acc: 0.4311 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# 6. Training\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint('model.h5', save_best_only=True, save_weights_only=False, monitor='val_loss'),\n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)]\n",
    "\n",
    "history = model.fit(X_tr, y_tr, epochs=50, batch_size=256, verbose=1, callbacks=callbacks, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c614336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Function for Text Generation using Beam Search\n",
    "def generate_seq_beam_search(model, mapping, seq_length, seed_text, n_chars, beam_width=3):\n",
    "    sequences = [{'seq': seed_text, 'score': 0.0}]\n",
    "    for _ in range(n_chars):\n",
    "        all_candidates = list()\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]['seq'], sequences[i]['score']\n",
    "            encoded = [mapping[char] for char in seq]\n",
    "            encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre').squeeze()\n",
    "            pred = model.predict(np.array([encoded]), verbose=0)\n",
    "            probas = np.exp(pred) / np.sum(np.exp(pred))\n",
    "            top_k = heapq.nlargest(beam_width, zip(probas[0], list(range(len(probas[0])))))\n",
    "            for j in range(len(top_k)):\n",
    "                score_, idx = top_k[j]\n",
    "                out_char = reverse_mapping[idx]\n",
    "                candidate = {'seq': seq + out_char, 'score': score - np.log(score_)}\n",
    "                all_candidates.append(candidate)\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup['score'], reverse=True)\n",
    "        sequences = ordered[:beam_width]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1a785eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juebes veinte tres de febrero de mil setecientosadnoresce dronegcteneosacraces\n"
     ]
    }
   ],
   "source": [
    "#Function Design for predicting missing text after a sentence\n",
    "seed_text = \"Juebes veinte tres de febrero de mil setecientos\".lower()\n",
    "num_chars_to_predict = 30\n",
    "results = generate_seq_beam_search(model, mapping, 30, seed_text, num_chars_to_predict)\n",
    "print(results[0]['seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f3414ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juebes veinterosellacle nterde mil setecientos\n"
     ]
    }
   ],
   "source": [
    "#Predicting missing text in a gap \n",
    "# Define the text before and after the gap\n",
    "text_before_gap = \"Juebes veinte\".lower()\n",
    "text_after_gap = \"de mil setecientos\".lower()\n",
    "\n",
    "# Estimate the gap length (this can be a rough estimate or based on context)\n",
    "# Here, I'm using an arbitrary value; adjust as needed\n",
    "gap_length_estimate = 15\n",
    "\n",
    "# Generate content for the gap\n",
    "results = generate_seq_beam_search(model, mapping, 30, text_before_gap, gap_length_estimate)\n",
    "predicted_gap_content = results[0]['seq'][len(text_before_gap):]\n",
    "\n",
    "# Merge the content\n",
    "completed_sentence = text_before_gap + predicted_gap_content + text_after_gap\n",
    "print(completed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "828c4375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   0.54      0.72      0.62     16590\n",
      "           a       0.34      0.37      0.35     10747\n",
      "           b       0.53      0.24      0.33      1440\n",
      "           c       0.30      0.16      0.21      4345\n",
      "           d       0.25      0.20      0.23      3124\n",
      "           e       0.40      0.37      0.39     10673\n",
      "           f       0.36      0.23      0.28      1014\n",
      "           g       0.47      0.33      0.39      1412\n",
      "           h       0.54      0.26      0.35       861\n",
      "           i       0.39      0.40      0.40      7961\n",
      "           j       0.35      0.33      0.34       847\n",
      "           k       0.25      0.06      0.10        34\n",
      "           l       0.39      0.28      0.33      4415\n",
      "           m       0.31      0.34      0.32      2953\n",
      "           n       0.41      0.46      0.43      6568\n",
      "           o       0.45      0.39      0.42      9577\n",
      "           p       0.31      0.51      0.39      2507\n",
      "           q       0.60      0.45      0.52       636\n",
      "           r       0.44      0.43      0.43      7194\n",
      "           s       0.42      0.46      0.44      8039\n",
      "           t       0.49      0.44      0.46      4172\n",
      "           u       0.55      0.54      0.55      3716\n",
      "           v       0.52      0.24      0.33      1116\n",
      "           w       0.00      0.00      0.00         3\n",
      "           x       0.48      0.66      0.56       232\n",
      "           y       0.80      0.30      0.44       361\n",
      "           z       0.68      0.40      0.51       785\n",
      "           á       0.46      0.15      0.22       217\n",
      "           é       0.49      0.44      0.47       332\n",
      "           í       0.57      0.46      0.51       250\n",
      "           ñ       0.60      0.57      0.58       348\n",
      "           ó       0.51      0.57      0.54       132\n",
      "           ú       0.12      0.05      0.07        19\n",
      "\n",
      "    accuracy                           0.43    112620\n",
      "   macro avg       0.43      0.36      0.38    112620\n",
      "weighted avg       0.43      0.43      0.42    112620\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suzreal\\miniconda3\\envs\\tf888\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Suzreal\\miniconda3\\envs\\tf888\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Suzreal\\miniconda3\\envs\\tf888\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Extract unique classes from y_true and y_pred\n",
    "unique_classes = np.union1d(y_true, y_pred)\n",
    "\n",
    "# Create target names from these unique classes\n",
    "target_names = [reverse_mapping[idx] for idx in unique_classes]\n",
    "\n",
    "# Compute Precision, Recall, and Accuracy\n",
    "report = classification_report(y_true, y_pred, labels=unique_classes, target_names=target_names)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
