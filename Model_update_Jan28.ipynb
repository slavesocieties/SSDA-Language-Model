{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f83a9118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import heapq\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9739a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Suzreal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "with open('training_update.txt', 'r', encoding='utf-8') as file:\n",
    "    training_data = file.read()\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(\"[^a-zA-Zñáéíóúü ]\", \" \", newString)\n",
    "    newString = re.sub('\\s+', ' ', newString)\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    long_words = [word for word in newString.split() if len(word) >= 3 and word not in stop_words]\n",
    "    return \" \".join(long_words).strip()\n",
    "\n",
    "def split_into_segments(input_data):\n",
    "    segments = input_data.split('</entry>')\n",
    "    cleaned_segments = [re.sub(r'<.*?>', '', segment).strip() for segment in segments if segment.strip()]\n",
    "    return cleaned_segments\n",
    "\n",
    "\n",
    "segments = split_into_segments(training_data)\n",
    "cleaned_segments = [text_cleaner(segment) for segment in segments]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "000e9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenizing the text into words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([cleaned_segments])\n",
    "encoded = tokenizer.texts_to_sequences([cleaned_segments])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8898721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Creating Sequences of Tokens\n",
    "sequence_length = 5  # Length of the word sequences\n",
    "sequences = [encoded[i - sequence_length:i+1] for i in range(sequence_length, len(encoded))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efd09acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Preparing the dataset\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc7d583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Defining the Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=sequence_length, trainable=True))\n",
    "model.add(GRU(150, return_sequences=False))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c67909a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "44/44 [==============================] - 4s 17ms/step - loss: 7.8065 - accuracy: 0.0015 - val_loss: 7.7547 - val_accuracy: 0.0000e+00 - lr: 0.0100\n",
      "Epoch 2/50\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 7.3360 - accuracy: 0.0160 - val_loss: 7.7294 - val_accuracy: 0.0043 - lr: 0.0100\n",
      "Epoch 3/50\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 5.3141 - accuracy: 0.1759 - val_loss: 6.9295 - val_accuracy: 0.1667 - lr: 0.0100\n",
      "Epoch 4/50\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 2.4756 - accuracy: 0.5009 - val_loss: 6.6217 - val_accuracy: 0.4188 - lr: 0.0100\n",
      "Epoch 5/50\n",
      "44/44 [==============================] - 0s 7ms/step - loss: 0.6850 - accuracy: 0.8705 - val_loss: 7.0923 - val_accuracy: 0.4449 - lr: 0.0100\n",
      "Epoch 6/50\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.1855 - accuracy: 0.9706 - val_loss: 7.2001 - val_accuracy: 0.4464 - lr: 0.0100\n",
      "Epoch 7/50\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9909 - val_loss: 7.3104 - val_accuracy: 0.4536 - lr: 0.0100\n",
      "Epoch 8/50\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0141 - accuracy: 0.9989 - val_loss: 7.3054 - val_accuracy: 0.4551 - lr: 0.0020\n",
      "Epoch 9/50\n",
      "44/44 [==============================] - 0s 8ms/step - loss: 0.0094 - accuracy: 0.9996 - val_loss: 7.3109 - val_accuracy: 0.4551 - lr: 0.0020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x210411be940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Training\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5),\n",
    "    ModelCheckpoint('model.h5', save_best_only=True, monitor='val_loss'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
    "]\n",
    "\n",
    "model.fit(X_tr, y_tr, epochs=50, batch_size=64, validation_data=(X_val, y_val), verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8fb61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Level Function\n",
    "def generate_seq_beam_search(model, tokenizer, seq_length, seed_text, num_words, beam_width=3):\n",
    "    sequences = [{'seq': seed_text, 'score': 1.0}]\n",
    "    for _ in range(num_words):\n",
    "        all_candidates = []\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]['seq'], sequences[i]['score']\n",
    "            if len(seq) < seq_length:\n",
    "                sequence = pad_sequences([seq], maxlen=seq_length, truncating='pre')\n",
    "            else:\n",
    "                sequence = np.array(seq[-seq_length:]).reshape(1, seq_length)\n",
    "            \n",
    "            preds = model.predict(sequence, verbose=0).flatten()\n",
    "            top_indices = np.argsort(preds)[-beam_width:]\n",
    "\n",
    "            for j in top_indices:\n",
    "                candidate = [num for num in seq] + [j]\n",
    "                candidate_score = score * preds[j]\n",
    "                all_candidates.append({'seq': candidate, 'score': candidate_score})\n",
    "\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup['score'], reverse=True)\n",
    "        sequences = ordered[:beam_width]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "336a0032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the texts before the gap: Jueves veinte de\n",
      "Enter the texts after the gap: mil setecientos\n",
      "jueves veinte de josef maria rangel congo maria josefa criolla ciudad havana veinte seis octubre mil ochocientos doce años haviendose leydo tres canonicas amonestaciones tres días festivos resultar impedimento licenciado don andres cascales beneficiado iglesia auxiliar santo angel custodio ciudad havana case vele ritual mente josef maria rangel congo viudo maria dolores esclavo don antonio rangel maria josefa luisa criolla ciudad hija legítima agustin rafaela esclava don manuel apesechea dhos contrayentes confesaron comulgaron examinados doctrina cristiana siendo testigos don josef rafael morales don juan escobar padrinos josef cruz yglesias maria lus dias firme licenciado andres cascales josef maria rangel congo maria josefa criolla ciudad havana veinte seis octubre mil ochocientos doce años haviendose leydo tres canonicas amonestaciones tres días festivos resultar impedimento licenciado don andres cascales beneficiado iglesia auxiliar santo angel custodio ciudad havana case vele ritual mente josef maria rangel congo viudo maria dolores esclavo don antonio rangel maria josefa luisa criolla ciudad hija legítima agustin rafaela esclava don manuel apesechea dhos contrayentes confesaron comulgaron examinados doctrina cristiana siendo testigos don josef rafael morales don juan escobar padrinos josef cruz yglesias maria lus dias firme licenciado andres cascales josef maria rangel congo maria josefa criolla ciudad havana veinte seis octubre mil ochocientos doce años haviendose leydo tres canonicas amonestaciones tres días festivos resultar impedimento licenciado don andres cascales beneficiado iglesia auxiliar santo angel custodio ciudad havana case vele ritual mente josef maria rangel congo viudo maria dolores esclavo don antonio rangel maria josefa luisa criolla ciudad hija legítima agustin rafaela esclava don manuel apesechea dhos contrayentes confesaron comulgaron examinados doctrina cristiana siendo testigos don josef rafael morales don juan escobar padrinos josef cruz yglesias maria lus dias firme licenciado andres cascales mil setecientos\n"
     ]
    }
   ],
   "source": [
    "def generate_text_for_gap(model, tokenizer, sequence_length, text_before_gap, text_after_gap, gap_length_estimate=3, beam_width=3):\n",
    "    #lower case\n",
    "    text_before_gap = text_before_gap.lower()\n",
    "    text_after_gap = text_after_gap.lower()\n",
    "\n",
    "    seed_text = text_before_gap\n",
    "    sequence_seed = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    seed_text_length = len(sequence_seed)\n",
    "\n",
    "    results = generate_seq_beam_search(model, tokenizer, sequence_length, sequence_seed, gap_length_estimate, beam_width)\n",
    "    best_sequence = results[0]['seq']\n",
    "\n",
    "    predicted_gap_content = ' '.join(tokenizer.index_word.get(idx, '') for idx in best_sequence[seed_text_length:])\n",
    "\n",
    "    completed_sentence = text_before_gap + ' ' + predicted_gap_content + ' ' + text_after_gap\n",
    "    return completed_sentence\n",
    "\n",
    "text_before_gap = input(\"Enter the texts before the gap: \")\n",
    "text_after_gap = input(\"Enter the texts after the gap: \")\n",
    "generated_text = generate_text_for_gap(model, tokenizer, sequence_length, text_before_gap, text_after_gap)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd579456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
