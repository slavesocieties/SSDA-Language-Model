{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hlqPyebDG5HO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vE5pcRzxHR8H"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "with open(\"data\\\\txt\\\\master.txt\", \"r\", encoding=\"utf-8\") as infile:\n",
    "    data_text = line\n",
    "\n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    long_words=[]\n",
    "    for i in newString.split():\n",
    "        if len(i)>3:                  \n",
    "            long_words.append(i)\n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "data_new = text_cleaner(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UFovlYuHjGh",
    "outputId": "c7b377a0-b943-4a35-bfa9-86395f0e2b7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 1072063\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing Part 2\n",
    "def create_seq(text, length = 30):\n",
    "    sequences = list()\n",
    "    for i in range(length, len(text)):\n",
    "        seq = text[i-length:i+1]\n",
    "        sequences.append(seq)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences\n",
    "\n",
    "sequences = create_seq(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ej1Rx_7YHpiB"
   },
   "outputs": [],
   "source": [
    "# Create a character mapping index\n",
    "chars = sorted(list(set(data_new)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "def encode_seq(seq):\n",
    "    sequences = list()\n",
    "    for line in seq:\n",
    "        encoded_seq = [mapping[char] for char in line]\n",
    "        sequences.append(encoded_seq)\n",
    "    return sequences\n",
    "\n",
    "sequences = encode_seq(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XCOLHL2yHsou"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Prepare the dataset\n",
    "vocab = len(mapping)\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQLCaogwH4mF",
    "outputId": "52607fb0-f50a-4cf3-eec6-888662ac2a03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 30, 50)            1350      \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 150)               90900     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 27)                4077      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96,327\n",
      "Trainable params: 96,327\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
    "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
    "model.add(Dense(vocab, activation='softmax'))\n",
    "print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer=Adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HCVthOJtIXpl",
    "outputId": "df06f563-1397-4809-a439-35041917bd92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3769/3769 [==============================] - 270s 71ms/step - loss: 1.9970 - acc: 0.3835 - val_loss: 2.0435 - val_acc: 0.3535\n",
      "Epoch 2/10\n",
      "3769/3769 [==============================] - 261s 69ms/step - loss: 1.9529 - acc: 0.3889 - val_loss: 1.6641 - val_acc: 0.4986\n",
      "Epoch 3/10\n",
      "3769/3769 [==============================] - 316s 84ms/step - loss: 1.5512 - acc: 0.5350 - val_loss: 1.2918 - val_acc: 0.6311\n",
      "Epoch 4/10\n",
      "3769/3769 [==============================] - 334s 89ms/step - loss: 1.3288 - acc: 0.6090 - val_loss: 1.2230 - val_acc: 0.6447\n",
      "Epoch 5/10\n",
      "3769/3769 [==============================] - 348s 92ms/step - loss: 1.2883 - acc: 0.6204 - val_loss: 1.2038 - val_acc: 0.6516\n",
      "Epoch 6/10\n",
      "3769/3769 [==============================] - 362s 96ms/step - loss: 1.2798 - acc: 0.6218 - val_loss: 1.1910 - val_acc: 0.6515\n",
      "Epoch 7/10\n",
      "3769/3769 [==============================] - 377s 100ms/step - loss: 1.2516 - acc: 0.6298 - val_loss: 1.1702 - val_acc: 0.6561\n",
      "Epoch 8/10\n",
      "3769/3769 [==============================] - 365s 97ms/step - loss: 1.2506 - acc: 0.6300 - val_loss: 1.1662 - val_acc: 0.6589\n",
      "Epoch 9/10\n",
      "3769/3769 [==============================] - 386s 102ms/step - loss: 1.2578 - acc: 0.6271 - val_loss: 1.1989 - val_acc: 0.6509\n",
      "Epoch 10/10\n",
      "3769/3769 [==============================] - 361s 96ms/step - loss: 1.2690 - acc: 0.6240 - val_loss: 1.1921 - val_acc: 0.6508\n"
     ]
    }
   ],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint('model.h5', save_best_only=True, \n",
    "             save_weights_only=False, monitor='val_loss')]\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_tr, y_tr, epochs=10, batch_size=256, #Manually set to 10 to make it run faster\n",
    "                    verbose=1, callbacks=callbacks, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NcOKTPxGImgI"
   },
   "outputs": [],
   "source": [
    "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
    "    import heapq\n",
    "    in_text = seed_text\n",
    "    predictions = []\n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(n_chars):\n",
    "        # encode the characters as integers\n",
    "        encoded = [mapping[char] for char in in_text]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre').squeeze()\n",
    "        # predict character\n",
    "        pred = model.predict(np.array([encoded]), verbose=0)\n",
    "        # applying softmax to convert output into probabilities\n",
    "        probas = np.exp(pred) / np.sum(np.exp(pred))\n",
    "        # getting top 3 predictions\n",
    "        top_3 = heapq.nlargest(3, zip(probas[0], list(range(len(probas[0])))))\n",
    "        # reverse map integer to character for each prediction and store them\n",
    "        for score, idx in top_3:\n",
    "            out_char = ''\n",
    "            for char, index in mapping.items():\n",
    "                if index == idx:\n",
    "                    out_char = char\n",
    "                    break\n",
    "            # store the prediction information\n",
    "            predictions.append({'score': float(score), 'token': idx, 'token_str': out_char, 'sequence': in_text + out_char})\n",
    "        # continue generating based on the top prediction\n",
    "        in_text += predictions[-1]['token_str']\n",
    "    return in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WqF5O2_JIqO7",
    "outputId": "1a6402b8-dcba-4c43-ee09-6ea461cd444f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domingo nueve de junio de mil seten fun simanasta\n"
     ]
    }
   ],
   "source": [
    "inp = \"Domingo nueve de Junio de mil sete\"\n",
    "\n",
    "print(generate_probs(model, mapping, 30, inp.lower(), 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
