{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-y_iKv2V2Fe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import heapq\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Data preprocessing\n",
        "with open(\"data\\\\txt\\\\master.txt\", \"r\", encoding=\"utf-8\") as infile:\n",
        "    for line in infile:\n",
        "        data_text = line\n",
        "\n",
        "def text_cleaner(text):\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zñáéíóúü]\", \" \", newString)\n",
        "    long_words=[]\n",
        "    for i in newString.split():\n",
        "        if len(i)>=3:\n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "data_new = text_cleaner(data_text)\n",
        "\n",
        "# Creating Sequences\n",
        "def create_seq(text, length = 30):\n",
        "    sequences = list()\n",
        "    for i in range(length, len(text)):\n",
        "        seq = text[i-length:i+1]\n",
        "        sequences.append(seq)\n",
        "    return sequences\n",
        "\n",
        "sequences = create_seq(data_new)\n",
        "\n",
        "# Character Mapping\n",
        "chars = sorted(list(set(data_new)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "def encode_seq(seq):\n",
        "    sequences = list()\n",
        "    for line in seq:\n",
        "        encoded_seq = [mapping[char] for char in line]\n",
        "        sequences.append(encoded_seq)\n",
        "    return sequences\n",
        "\n",
        "sequences = encode_seq(sequences)\n",
        "\n",
        "# Preparing the dataset\n",
        "vocab = len(mapping)\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab)\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Defining the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
        "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
        "model.add(Dense(vocab, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer=Adam(learning_rate=0.01))\n",
        "\n",
        "# Training\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
        "             ModelCheckpoint('model.h5', save_best_only=True,\n",
        "             save_weights_only=False, monitor='val_loss')]\n",
        "\n",
        "history = model.fit(X_tr, y_tr, epochs=50, batch_size=256,\n",
        "                    verbose=1, callbacks=callbacks, validation_data=(X_val, y_val))\n",
        "\n",
        "# Function for text generation using beam search\n",
        "def generate_seq_beam_search(model, mapping, seq_length, seed_text, n_chars, beam_width=3):\n",
        "    sequences = [{'seq': seed_text, 'score': 0.0}]\n",
        "    for _ in range(n_chars):\n",
        "        all_candidates = list()\n",
        "        for i in range(len(sequences)):\n",
        "            seq, score = sequences[i]['seq'], sequences[i]['score']\n",
        "            encoded = [mapping[char] for char in seq]\n",
        "            encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre').squeeze()\n",
        "            pred = model.predict(np.array([encoded]), verbose=0)\n",
        "            probas = np.exp(pred) / np.sum(np.exp(pred))\n",
        "            top_k = heapq.nlargest(beam_width, zip(probas[0], list(range(len(probas[0])))))\n",
        "            for j in range(len(top_k)):\n",
        "                score_, idx = top_k[j]\n",
        "                out_char = ''\n",
        "                for char, index in mapping.items():\n",
        "                    if index == idx:\n",
        "                        out_char = char\n",
        "                        break\n",
        "                candidate = {'seq': seq + out_char, 'score': score - np.log(score_)}\n",
        "                all_candidates.append(candidate)\n",
        "        ordered = sorted(all_candidates, key=lambda tup:tup['score'])\n",
        "        sequences = ordered[:beam_width]\n",
        "    return sequences\n",
        "\n",
        "# Generate text\n",
        "print(generate_seq_beam_search(model, mapping, 30, \"Domingo nueve de Junio de mil sete\".lower(), 15))\n"
      ]
    }
  ]
}