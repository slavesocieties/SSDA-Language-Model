{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6pYR8afc6pgz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_text = \"Juana. esclava Domingo veinte y dos de y nueve yo Thomas de Orvera baptize, y pusse santos oleos a Juana de nacion Mina esclava de Juan Joseph de Justis fueron sus P.P. Joseph Salcedo y Ana de Santiago su mugger, y lo firmé. Thomas de Orvera Paula. esclava Juebes veinte y tres de febrero de mil setecientos. y diez y nueve Yo Thomas de Orvera baptizé, y pusse los santos15 oleos á Paula h. l.16 de Juan Joseph, y Maria Josepha esclavos del Capitán Don Luis Hurtado de Mendoza fue su padrino Bartholome Rixo, y lo firmé. Thomas de Orvera Maria esclava Miercoles primero de febrero de mil sietectos y diez y nueve Yo Thomas de Orvera baptizé, y pusse los santos oleos á Maria h. l. de Juan, y Josepha esclavos del Capitán Antonio Benites fue su Madrina Ysabel Mendez, y lo firmé. Thomas de Orvera Bernardo esclavo Domingo nueve de Abril de mil setecientos y diez y nueve Yo Thomas de Orvera baptize, y pusse los santos oleos á Bernardo negro adulto de nacion Carabalí esclavo de Don Juan Joseph de Justis fue su Padre. Andres de Morales, y lo firmé. Thomas de Orvera Francisco esclavo Abril de mil setecientos y diez, y nueve o Thmas de vera bapizé, y pusse los s.tos oleo a Francisco negro adulto de nacion temo esclavo de Don Ju Joseph de Justis fue su Padre. Pedro Suares, y lo firmé. Thomas de Orvera Antonio esclavo Domingo nueve de Abril de mil setecientos y diez y nueve Yo Thomas de Orvera baptize, y pusse los santos oleos á Antonio negro adulto de nacion Carabalí esclavo de Don Juan Joseph de Justis fue su Padre. Joseph de Soto, y lo firmé. Thomas de Orvera Antonia esclava Domingo nueve de Abril de mil setecientos y diez, y nueve Yo Thomas de Orvera baptize, y pusse los santos oleos á Antonia negra adulta de nacion Mina esclava de Don Juan Joseph de Justis fue su Padre. Joseph Salcedo, y lo firmé. Thomas de Orvera Maria Luisa esclava Domingo nueve de Abril de mil setecientos y diez, y nueve Yo Thomas de Orvera baptize, y puse los santos oleos á Maria Luisa de nacion Lucumí negra adulta esclava de Don Juan Joseph de Justis fue su Padre. Jacinto de Castro, y lo firmé.\""
      ],
      "metadata": {
        "id": "hMQdu7S1-P9i"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def text_cleaner(text):\n",
        "    # lower case text\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    # remove punctuations\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    long_words=[]\n",
        "    # remove short word\n",
        "    for i in newString.split():\n",
        "        if len(i)>=3:                  \n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "# preprocess the text\n",
        "data_new = text_cleaner(data_text)"
      ],
      "metadata": {
        "id": "TTl0ZOXEAhRi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_seq(text):\n",
        "    length = 30\n",
        "    sequences = list()\n",
        "    for i in range(length, len(text)):\n",
        "        # select sequence of tokens\n",
        "        seq = text[i-length:i+1]\n",
        "        # store\n",
        "        sequences.append(seq)\n",
        "    print('Total Sequences: %d' % len(sequences))\n",
        "    return sequences\n",
        "\n",
        "# create sequences   \n",
        "sequences = create_seq(data_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caLbbDKdAmOb",
        "outputId": "6ca3d757-a198-4f52-fa89-f5bcf030138c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 1655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a character mapping index\n",
        "chars = sorted(list(set(data_new)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "def encode_seq(seq):\n",
        "    sequences = list()\n",
        "    for line in seq:\n",
        "        # integer encode line\n",
        "        encoded_seq = [mapping[char] for char in line]\n",
        "        # store\n",
        "        sequences.append(encoded_seq)\n",
        "    return sequences\n",
        "\n",
        "# encode the sequences\n",
        "sequences = encode_seq(sequences)"
      ],
      "metadata": {
        "id": "mtFFrLyxAxLQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# vocabulary size\n",
        "vocab = len(mapping)\n",
        "sequences = np.array(sequences)\n",
        "# create X and y\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "# one hot encode y\n",
        "y = to_categorical(y, num_classes=vocab)\n",
        "# create train and validation sets\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh2YTVUfA4ye",
        "outputId": "0235ee83-bb40-4261-d48d-5a5017947482"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (1489, 30) Val shape: (166, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
        "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
        "model.add(Dense(vocab, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
        "# fit the model\n",
        "model.fit(X_tr, y_tr, epochs=20, verbose=2, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1juZ6ZrJA9xg",
        "outputId": "c0b71095-7490-426d-a4be-7d94567efe38"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 30, 50)            1200      \n",
            "                                                                 \n",
            " gru_3 (GRU)                 (None, 150)               90900     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 24)                3624      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 95,724\n",
            "Trainable params: 95,724\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "47/47 - 8s - loss: 2.9709 - acc: 0.1444 - val_loss: 2.9246 - val_acc: 0.1687 - 8s/epoch - 171ms/step\n",
            "Epoch 2/20\n",
            "47/47 - 4s - loss: 2.7723 - acc: 0.1713 - val_loss: 2.7626 - val_acc: 0.2410 - 4s/epoch - 84ms/step\n",
            "Epoch 3/20\n",
            "47/47 - 5s - loss: 2.5745 - acc: 0.2626 - val_loss: 2.4657 - val_acc: 0.2892 - 5s/epoch - 114ms/step\n",
            "Epoch 4/20\n",
            "47/47 - 4s - loss: 2.2460 - acc: 0.3123 - val_loss: 2.1906 - val_acc: 0.3253 - 4s/epoch - 84ms/step\n",
            "Epoch 5/20\n",
            "47/47 - 4s - loss: 1.9751 - acc: 0.3747 - val_loss: 2.0105 - val_acc: 0.3494 - 4s/epoch - 86ms/step\n",
            "Epoch 6/20\n",
            "47/47 - 5s - loss: 1.7891 - acc: 0.4338 - val_loss: 1.8256 - val_acc: 0.4518 - 5s/epoch - 113ms/step\n",
            "Epoch 7/20\n",
            "47/47 - 4s - loss: 1.5637 - acc: 0.5205 - val_loss: 1.6357 - val_acc: 0.5361 - 4s/epoch - 84ms/step\n",
            "Epoch 8/20\n",
            "47/47 - 4s - loss: 1.3904 - acc: 0.5756 - val_loss: 1.5340 - val_acc: 0.5663 - 4s/epoch - 82ms/step\n",
            "Epoch 9/20\n",
            "47/47 - 5s - loss: 1.2284 - acc: 0.6273 - val_loss: 1.4039 - val_acc: 0.6506 - 5s/epoch - 114ms/step\n",
            "Epoch 10/20\n",
            "47/47 - 4s - loss: 1.0904 - acc: 0.6917 - val_loss: 1.3033 - val_acc: 0.6747 - 4s/epoch - 85ms/step\n",
            "Epoch 11/20\n",
            "47/47 - 4s - loss: 0.9878 - acc: 0.7273 - val_loss: 1.2795 - val_acc: 0.7048 - 4s/epoch - 84ms/step\n",
            "Epoch 12/20\n",
            "47/47 - 5s - loss: 0.8989 - acc: 0.7488 - val_loss: 1.2137 - val_acc: 0.7289 - 5s/epoch - 111ms/step\n",
            "Epoch 13/20\n",
            "47/47 - 4s - loss: 0.8280 - acc: 0.7770 - val_loss: 1.1622 - val_acc: 0.7410 - 4s/epoch - 84ms/step\n",
            "Epoch 14/20\n",
            "47/47 - 4s - loss: 0.7367 - acc: 0.7945 - val_loss: 1.1310 - val_acc: 0.7470 - 4s/epoch - 89ms/step\n",
            "Epoch 15/20\n",
            "47/47 - 5s - loss: 0.6880 - acc: 0.8167 - val_loss: 1.1001 - val_acc: 0.7530 - 5s/epoch - 104ms/step\n",
            "Epoch 16/20\n",
            "47/47 - 4s - loss: 0.6263 - acc: 0.8247 - val_loss: 1.0746 - val_acc: 0.7410 - 4s/epoch - 84ms/step\n",
            "Epoch 17/20\n",
            "47/47 - 4s - loss: 0.5728 - acc: 0.8469 - val_loss: 1.0786 - val_acc: 0.7711 - 4s/epoch - 91ms/step\n",
            "Epoch 18/20\n",
            "47/47 - 5s - loss: 0.5300 - acc: 0.8556 - val_loss: 1.0835 - val_acc: 0.7831 - 5s/epoch - 103ms/step\n",
            "Epoch 19/20\n",
            "47/47 - 4s - loss: 0.4953 - acc: 0.8583 - val_loss: 1.0630 - val_acc: 0.7651 - 4s/epoch - 82ms/step\n",
            "Epoch 20/20\n",
            "47/47 - 4s - loss: 0.4523 - acc: 0.8791 - val_loss: 1.0279 - val_acc: 0.7831 - 4s/epoch - 92ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f57678765c0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of characters\n",
        "    for _ in range(n_chars):\n",
        "        # encode the characters as integers\n",
        "        encoded = [mapping[char] for char in in_text]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # one hot encode\n",
        "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
        "        # predict character\n",
        "        yhat = np.argmax(model.predict(encoded), axis=-1)\n",
        "        # reverse map integer to character\n",
        "        out_char = ''\n",
        "        for char, index in mapping.items():\n",
        "            if index == yhat:\n",
        "                out_char = char\n",
        "                break\n",
        "        # append to input\n",
        "        in_text += out_char\n",
        "    return in_text"
      ],
      "metadata": {
        "id": "A7nNGGU1FHAS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_seq(model, mapping, 30, 'juana esclava', 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "CdwBhtBFFj7G",
        "outputId": "9d154a76-b339-4e41-be9f-e8d55d06a390"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-bb6f3e5d2828>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'juana esclava'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-9492bb6086b7>\u001b[0m in \u001b[0;36mgenerate_seq\u001b[0;34m(model, mapping, seq_length, seed_text, n_chars)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# predict character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# reverse map integer to character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mout_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_3' (type Sequential).\n    \n    Input 0 of layer \"gru_3\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 30, 24, 50)\n    \n    Call arguments received by layer 'sequential_3' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 30, 24), dtype=float32)\n      • training=False\n      • mask=None\n"
          ]
        }
      ]
    }
  ]
}