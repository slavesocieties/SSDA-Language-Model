{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_QI_PxNuMe0",
        "outputId": "2954fe52-e994-423e-c14a-7137fcc2744c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 387\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 30, 50)            1300      \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 150)               90900     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 26)                3926      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 96,126\n",
            "Trainable params: 96,126\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 5s 971ms/step - loss: 3.2429 - acc: 0.0891 - val_loss: 3.0668 - val_acc: 0.1282\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 1s 330ms/step - loss: 3.6248 - acc: 0.1609 - val_loss: 3.2186 - val_acc: 0.1026\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 191ms/step - loss: 3.0763 - acc: 0.1236 - val_loss: 3.0262 - val_acc: 0.1026\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 3.0113 - acc: 0.2040 - val_loss: 2.9895 - val_acc: 0.2051\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 196ms/step - loss: 2.9599 - acc: 0.2644 - val_loss: 2.8824 - val_acc: 0.1538\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 190ms/step - loss: 2.8387 - acc: 0.2299 - val_loss: 2.7499 - val_acc: 0.1538\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 2.6597 - acc: 0.2213 - val_loss: 2.6933 - val_acc: 0.1538\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 2.5595 - acc: 0.2328 - val_loss: 2.5834 - val_acc: 0.3077\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 2.4258 - acc: 0.3161 - val_loss: 2.4917 - val_acc: 0.3590\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 193ms/step - loss: 2.3140 - acc: 0.3534 - val_loss: 2.4573 - val_acc: 0.3846\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 2.1991 - acc: 0.3879 - val_loss: 2.4105 - val_acc: 0.3590\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 2.0793 - acc: 0.4080 - val_loss: 2.3674 - val_acc: 0.3333\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 191ms/step - loss: 1.9572 - acc: 0.4224 - val_loss: 2.2975 - val_acc: 0.3846\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 1.8359 - acc: 0.4511 - val_loss: 2.2761 - val_acc: 0.3590\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 1.7336 - acc: 0.4540 - val_loss: 2.3034 - val_acc: 0.3590\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 182ms/step - loss: 1.6400 - acc: 0.4655 - val_loss: 2.3045 - val_acc: 0.3846\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 1.5326 - acc: 0.5230 - val_loss: 2.3422 - val_acc: 0.3846\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 188ms/step - loss: 1.4488 - acc: 0.5489 - val_loss: 2.3841 - val_acc: 0.3333\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 187ms/step - loss: 1.3516 - acc: 0.5575 - val_loss: 2.3769 - val_acc: 0.4103\n",
            "domingo nueve de junio de mil setehclireqmoreseva\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Data preprocessing\n",
        "data_text = \"\"\"\n",
        "Juana. esclava\n",
        "Domingo veinte y dos de \n",
        "y nueve yo Thomas de Orvera baptize, y pusse \n",
        "santos oleos a Juana de nacion Mina esclava de\n",
        "Juan Joseph de Justis fueron sus P.P. Joseph Salcedo\n",
        "y Ana de Santiago su mugger, y lo firmé.\n",
        "Lunes veinte y ocho de Diziembre de mil setecientos y veinte y dos as Yo\n",
        "Thomas de Orvera cura B.do de la Ygla. Parroq.l de San Carlos de\n",
        "Matanzas Baptize y puse los Ss. Oleos á Thomas negro adulto\n",
        "n.on Kongo esclavo de Don Juan Joseph de Justis fue su Padre. Joseph n.on\n",
        "Inglés esclavo del dho Don Juan, y lo firmé.\n",
        "\"\"\"\n",
        "def text_cleaner(text):\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zñáéíóúü]\", \" \", newString)\n",
        "    long_words=[]\n",
        "    for i in newString.split():\n",
        "        if len(i)>=3:                  \n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "data_new = text_cleaner(data_text)\n",
        "\n",
        "# Creating Sequences\n",
        "def create_seq(text, length = 30):\n",
        "    sequences = list()\n",
        "    for i in range(length, len(text)):\n",
        "        seq = text[i-length:i+1]\n",
        "        sequences.append(seq)\n",
        "    print('Total Sequences: %d' % len(sequences))\n",
        "    return sequences\n",
        "\n",
        "sequences = create_seq(data_new)\n",
        "\n",
        "# Character Mapping\n",
        "chars = sorted(list(set(data_new)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "def encode_seq(seq):\n",
        "    sequences = list()\n",
        "    for line in seq:\n",
        "        encoded_seq = [mapping[char] for char in line]\n",
        "        sequences.append(encoded_seq)\n",
        "    return sequences\n",
        "\n",
        "sequences = encode_seq(sequences)\n",
        "\n",
        "# Preparing the dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "vocab = len(mapping)\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab)\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Defining the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
        "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
        "model.add(Dense(vocab, activation='softmax'))\n",
        "print(model.summary())\n",
        "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer=Adam(learning_rate=0.01))\n",
        "\n",
        "# Training\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
        "             ModelCheckpoint('model.h5', save_best_only=True, \n",
        "             save_weights_only=False, monitor='val_loss')]\n",
        "\n",
        "history = model.fit(X_tr, y_tr, epochs=50, batch_size=256,\n",
        "                    verbose=1, callbacks=callbacks, validation_data=(X_val, y_val))\n",
        "\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "    import heapq\n",
        "    in_text = seed_text\n",
        "    predictions = []\n",
        "    # generate a fixed number of characters\n",
        "    for _ in range(n_chars):\n",
        "        # encode the characters as integers\n",
        "        encoded = [mapping[char] for char in in_text]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre').squeeze()\n",
        "        # predict character\n",
        "        pred = model.predict(np.array([encoded]), verbose=0)\n",
        "        # applying softmax to convert output into probabilities\n",
        "        probas = np.exp(pred) / np.sum(np.exp(pred))\n",
        "        # getting top 3 predictions\n",
        "        top_3 = heapq.nlargest(3, zip(probas[0], list(range(len(probas[0])))))\n",
        "        # reverse map integer to character for each prediction and store them\n",
        "        for score, idx in top_3:\n",
        "            out_char = ''\n",
        "            for char, index in mapping.items():\n",
        "                if index == idx:\n",
        "                    out_char = char\n",
        "                    break\n",
        "            # store the prediction information\n",
        "            predictions.append({'score': float(score), 'token': idx, 'token_str': out_char, 'sequence': in_text + out_char})\n",
        "        # continue generating based on the top prediction\n",
        "        in_text += predictions[-1]['token_str']\n",
        "    return in_text\n",
        "\n",
        "\n",
        "print(generate_seq(model, mapping, 30, \"Domingo nueve de Junio de mil sete\".lower(), 15))\n"
      ]
    }
  ]
}